pip install transformers datasets torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load pre-trained model and tokenizer
model_name = 'gpt2'
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
from datasets import load_dataset

# Load your dataset
dataset = load_dataset('text', data_files={'train': 'path_to_your_dataset.txt'})

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',          # Output directory
    num_train_epochs=3,              # Number of training epochs
    per_device_train_batch_size=4,   # Batch size per device during training
    save_steps=10_000,                # Save checkpoint every 10,000 steps
    save_total_limit=2,              # Limit the number of checkpoints
)
trainer = Trainer(
    model=model,                         # The model to be trained
    args=training_args,                  # Training arguments
    train_dataset=tokenized_datasets['train'],  # Training dataset
)
trainer.train()
model.save_pretrained('./fine-tuned-model')
tokenizer.save_pretrained('./fine-tuned-model')
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load the fine-tuned model
model = GPT2LMHeadModel.from_pretrained('./fine-tuned-model')
tokenizer = GPT2Tokenizer.from_pretrained('./fine-tuned-model')

# Define a prompt
prompt = "Once upon a time"
inputs = tokenizer(prompt, return_tensors='pt')

# Generate text
outputs = model.generate(inputs['input_ids'], max_length=100, num_return_sequences=1)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(generated_text)
